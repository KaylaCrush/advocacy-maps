{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "class LobbyingDataPage:\n",
    "    lobbying_file = '../data/lobbying.csv'\n",
    "    compensation_file = '../data/compensation.csv'\n",
    "\n",
    "    def __init__(self, html):\n",
    "        self.html = html\n",
    "        self.soup = bs(self.html,'html.parser')\n",
    "        self.company_name = self.get_company_name()\n",
    "        self.date_range = self.get_date_range()\n",
    "        \n",
    "        self.lobbying_data = self.extract_to_dataframe()\n",
    "        self.compensation_data = self.extract_compensation_data()\n",
    "\n",
    "    def save(self):\n",
    "        self.write_data(LobbyingDataPage.lobbying_file, self.lobbying_data)\n",
    "        self.write_data(LobbyingDataPage.compensation_file, self.compensation_data)\n",
    "\n",
    "    def write_data(self, file_path, dataframe):\n",
    "        write = True\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, mode = 'r') as f:\n",
    "                for line in f:\n",
    "                    if self.company_name in line and self.date_range in line:\n",
    "                        print('Data already present in ' + file_path)\n",
    "                        write = False\n",
    "                        break\n",
    "        if write:\n",
    "            dataframe.to_csv(file_path, mode ='a',header=not os.path.exists(file_path), index=False)\n",
    "            #with open(file_path, mode = 'w') as f:\n",
    "            #    f.write(dataframe.to_csv(file_path, mode='a',header=not os.path.exists(file_path)))\n",
    "\n",
    "    def get_date_range(self):\n",
    "        return self.soup.find('span', {'id': 'ContentPlaceHolder1_lblYear'}).text\n",
    "\n",
    "    def get_company_name(self):\n",
    "        return self.soup.find('span', {'id': 'ContentPlaceHolder1_ERegistrationInfoReview1_lblEntityCompany'}).text\n",
    "\n",
    "    def pull_data(self):\n",
    "        headers={\"User-Agent\": \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\"}\n",
    "        result = requests.get(self.url, headers=headers)\n",
    "        result.raise_for_status()\n",
    "        return result.content\n",
    "\n",
    "    def prep_tables(self):\n",
    "        some_tables = self.soup.find_all('tr', {'style': 'vertical-align: top'})\n",
    "        split_tables = [table for table in some_tables if 'Lobbyist: ' in table.text][0].text.split('Lobbyist: ')\n",
    "        the_tables = [entry for entry in split_tables if entry.strip() and 'House / Senate' in entry]\n",
    "\n",
    "        clean_tables = []\n",
    "        for table in the_tables:\n",
    "            clean_table = [line for line in table.split('\\n') if line] # divide by lines and remove empties\n",
    "            clean_table = clean_table[:clean_table.index('\\xa0\\xa0\\xa0')] # Remove ending cruft\n",
    "            clean_tables.append(clean_table)\n",
    "\n",
    "        return clean_tables\n",
    "    \n",
    "    def extract_to_dataframe(self):\n",
    "        good_soup = self.prep_tables()\n",
    "        row_dicts = []\n",
    "        for table in good_soup:\n",
    "            lobbyist_name = table[0]\n",
    "            client_name = table[2]\n",
    "            table_start_index = table.index('House / SenateBill Number or Agency NameBill title or activityAgent positionAmountDirect business association')+1\n",
    "            table_data = table[table_start_index:]\n",
    "            i=0\n",
    "            while i <= len(table_data)-7:\n",
    "                row_dicts.append({'LobbyingEntity': self.company_name,\n",
    "                                'DateRange': self.date_range,\n",
    "                                'Lobbyist': lobbyist_name, \n",
    "                                'Client': client_name, \n",
    "                                'House/Senate': table_data[i].strip(), \n",
    "                                'BillNumber':table_data[i+1].strip(), \n",
    "                                'BillActivity':table_data[i+2].strip(),\n",
    "                                'AgentPosition': table_data[i+3].strip(), \n",
    "                                'Amount': table_data[i+5].strip(), \n",
    "                                'DirectBusinessAssosciation': table_data[i+7].strip()})\n",
    "                i=i+8\n",
    "\n",
    "        return pd.DataFrame(row_dicts)\n",
    "\n",
    "    def extract_compensation_data(self):\n",
    "        compensation_table = self.soup.find('table', {'id': 'ContentPlaceHolder1_DisclosureReviewDetail1_grdvClientPaidToEntity'})\n",
    "        temp_list = [line.strip() for line in compensation_table.text.split('\\n') if line.strip()][1:-2]\n",
    "        temp_dict_list = []\n",
    "        it = iter(temp_list)\n",
    "        for i in it:\n",
    "            temp_dict_list.append({'LobbyingEntity': self.company_name, 'DateRange':self.date_range, 'Client': i, 'Amount':next(it)})\n",
    "\n",
    "        return pd.DataFrame(temp_dict_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save(html_list):\n",
    "    for html in html_list:\n",
    "        LobbyingDataPage(html).save()\n",
    "\n",
    "def pull_data(url = testing_url):\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\"}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    result.raise_for_status()\n",
    "    return result.content\n",
    "\n",
    "def download_html_list(url_list):\n",
    "    html_list = []\n",
    "    for url in url_list:\n",
    "        html_list.append(pull_data(url))\n",
    "    return html_list\n",
    "\n",
    "def save_data_from_url_list(url_list):\n",
    "    extract_and_save(download_html_list(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://www.sec.state.ma.us/LobbyistPublicSearch/CompleteDisclosure.aspx?sysvalue=Tcg7Il3rjW5sIbUrwbcVKSmBfkLmYFdnf8xTcXfPeaK3i269EErp97So5FUc/X23',\n",
    "            'https://www.sec.state.ma.us/LobbyistPublicSearch/CompleteDisclosure.aspx?sysvalue=Tcg7Il3rjW5sIbUrwbcVKV8k15XmkYG2WIUeIEwL/REZBWdbh+QzQDTofQB05tTR']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "300fbccfaa1afd04bb86653fbdee52f26c731e37f5150715ebdacf05ee18f2ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
